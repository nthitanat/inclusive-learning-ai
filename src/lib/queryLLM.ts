import { ChatOpenAI } from "@langchain/openai";
import { createStuffDocumentsChain } from "langchain/chains/combine_documents";
import { docsQuery } from "@/lib/docsQuery";
import { getChatPromptTemplate } from "@/lib/chatPromptTemplates";
const OPENAI_API_KEY = process.env.OPENAI_API_KEY;
const promptType = "query";
export async function callQueryLLM(task: string, query: string, needToRetrieve: boolean) {
  console.log("üîç Calling Simple LLM with prompt:");
  try {
    const chatModel = new ChatOpenAI({
      model: "gpt-4o",
      temperature: 0.7,
      openAIApiKey: OPENAI_API_KEY,
    });
    if (!needToRetrieve) {
      console.log("‚ÑπÔ∏è Skipping retrieval, using LLM directly");
      const response = await chatModel.invoke(task);
      console.log("‚úÖ LLM response (no retrieval):", response.content);
      return response.content;
    }

    // Retrieval and document chain process
    const lessonPlanGuidelinePath = process.cwd() + "/src/data/curriculum.csv";
    const curriculumQuery = await docsQuery(lessonPlanGuidelinePath);
    const curriculumDocs = await curriculumQuery.getRelevantDocuments(task);
    console.log("üì• Retrieved relevant guideline docs");

    const prompt = getChatPromptTemplate(promptType, query);

    const documentChain = await createStuffDocumentsChain({
      llm: chatModel,
      prompt,
    });

    const response = await documentChain.invoke({
      task: task,
      context: curriculumDocs,
    });

    console.log("‚úÖ LLM response:", response);
    return response;
  } catch (error) {
    console.error("‚ùå LangChain/OpenAI Error:", error);
    throw new Error("Failed to get AI response");
  }
}
